{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 1. Importing pVCFs into Hail and running genotype QC\n","\n","This script should be run on one chromosome at a time, so the most important part of this is defining the chromosome you're working with. \n","\n","Below is the instance type used for each chromosome and time taken to run this script. Often, the jupyterlab monitoring page on the RAP will say a limited amount of the compute power/instance has been used, but this isn't the case as jobs would be failing with less power than this. I think the Spark UI is a much more informative way to examine your scripts running, and you access this using this link: https://job-{jobid}.dnanexus.cloud:8081/jobs/ . You can get the job ID from the jupyternotebook instance you're running from, which will be https://job-{jobid}.dnanexus.cloud . The Spark UI wont work until you have initiated spark (run the first block of script below). \n","\n","For this stage, I generally used mem3_ssd1_v2_x8 instances, increasing the number of nodes as the number of partitions and size of the chromosome increased. Instances with 8 workers are recommended by Hail as these are what Hail is developed and tested on. The UKBB RAP allows a maximum of 100 nodes and therefore for larger chromosomes, I needed to increase to a mem3_ssd1_v2_x16 instance to enable the number of workers required for the job to be completed in a similar time to previous nodes (~2hours). Whilst the mem3 instance appears to be more memory than is typically required (it offers 62GB per worker, and the spark UI suggests the peak memory usage is just over 30GB per worker), the option below it (mem2, which offers 32GB per worker) often lead to jobs failing when I was testing this script, and was therefore not usable. \n","\n","Finally, for chromosomes 1 and 2, more memory per node than was offered by the mem3 instances was required. These chromosomes thus required the only mem4 option available (mem4_ssd1_x128) which was significantly more costly. I later had issues with chr1 being too large and therefore split it into two halves, so I would recommend doing this at this stage and using the mem3 instance instead to decrease processing costs.\n","\n","Numbers of variants, instances, nodes, times and costs are lister per chromosome below, but from my understanding of Hail time could have been reduced by increasing the number of nodes used. \n","- Chromosome 1: ~2.28 million loci, using a mem4_ssd1_x128 instance with 12 nodes, this script took ~ hrs, costing £320.\n","- Chromosome 2: ~1.68 million loci, using a mem4_ssd1_x128 instance with 11 nodes, this script took ~ 3.5 hrs, costing £280.\n","- Chromosome 3: ~1.34 million loci, using a mem3_ssd1_v2_x16 instance with 85 nodes, this script took ~ 1.7 hrs, costing £85.\n","- Chromosome 4: ~920k loci, using a mem3_ssd1_v2_x8 instance with 100 nodes, this script took ~1.8 hrs, costing £53.\n","- Chromosome 5: ~1.02 million loci, using a mem3_ssd1_v2_x16 instance with 60 nodes, this script took ~ 1.8 hrs, costing £63.\n","- Chromosome 6: ~1.14 million loci, using a mem3_ssd1_v2_x16 instance with 60 nodes, this script took ~ 1.9 hrs, costing £67.\n","- Chromosome 7: ~1.09 million loci, using a mem3_ssd1_v2_x16 instance with 60 nodes, this script took ~ 1.8 hrs, costing £65.\n","- Chromosome 8: ~820k loci, using a mem3_ssd1_v2_x8 instance with 70 nodes, this script took ~2.3 hrs, costing £46.\n","- Chromosome 9: ~980k loci, using a mem3_ssd1_v2_x8 instance with 100 nodes, this script took ~2 hrs, costing £58.\n","- Chromosome 10: ~940k loci, using a mem3_ssd1_v2_x8 instance with 100 nodes, this script took ~1.9 hrs, costing £55.\n","- Chromosome 11: ~1.34 million loci, using a mem3_ssd1_v2_x16 instance with 85 nodes, this script took ~1.8 hrs, costing £89.\n","- Chromosome 12: ~1.22 million loci, using a mem3_ssd1_v2_x16 instance with 80 nodes, this script took ~1.8 hrs, costing £72.\n","- Chromosome 13: ~410k loci, using a mem3_ssd1_v2_x8 instance with 50 nodes, this script took ~1.6 hrs, costing £24.\n","- Chromosome 14: ~710k loci, using a mem3_ssd1_v2_x8 instance with 70 nodes, this script took ~2 hrs, costing £40.\n","- Chromosome 15: ~790k loci, using a mem3_ssd1_v2_x8 instance with 70 nodes, this script took ~2.2 hrs, costing £44.\n","- Chromosome 16: ~1.07 million loci, using a mem3_ssd1_v2_x16 instance with 80 nodes, this script took ~1.6 hrs, costing £68.\n","- Chromosome 17: ~1.32 million loci, using a mem3_ssd1_v2_x16 instance with 80 nodes, this script took ~2.1 hrs, costing £84.\n","- Chromosome 18: ~370k loci, using a mem3_ssd1_v2_x8 instance with 20 nodes, this script took ~ 2.8 hrs, costing £15.\n","- Chromosome 19: ~1.49 million loci, using a mem3_ssd1_v2_x16 instance with 85 nodes, this script took ~1.9 hrs, costing £95.\n","- Chromosome 20: ~580k loci, using a mem3_ssd1_v2_x8 instance with 50 nodes, this script took ~2.1hrs, costing £31.\n","- Chromosome 21: ~240k loci, using a mem3_ssd1_v2_x8 instance with 20 nodes, this script took ~2.4hrs, costing £14.\n","- Chromosome 22: ~520k loci, using a mem3_ssd1_v2_x8 instance with 50 nodes, this script took ~2 hrs, costing £29.\n","- Chromosome 23(x): ~570k loci, using a mem3_ssd1_v2_x8 instance with 70 nodes, this script took ~ 1.5hrs, costing £30. \n","- Chromosome 24(y): ~10k loci, using a mem2_ssd1_v2_x8 instance with 2 nodes, this script took < 1 hour, costing £0.45. \n","\n","Matrix tables are saved out using DNAX. The matrix tables formed from this stage total 450GB. \n","\n","\n","## Set up environment\n","Make sure you run this block only once. You'll get errors if you try to initialise Hail multiple times. If you do do this, you'll need to restart the kernel, and then initialise Hail only once. \n"]},{"cell_type":"code","execution_count":1,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["pip-installed Hail requires additional configuration options in Spark referring\n","  to the path to the Hail Python module directory HAIL_DIR,\n","  e.g. /path/to/python/site-packages/hail:\n","    spark.jars=HAIL_DIR/backend/hail-all-spark.jar\n","    spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar\n","    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.2.3\n","SparkUI available at http://ip-10-60-8-105.eu-west-2.compute.internal:8081\n","Welcome to\n","     __  __     <>__\n","    / /_/ /__  __/ /\n","   / __  / _ `/ / /\n","  /_/ /_/\\_,_/_/_/   version 0.2.116-cd64e0876c94\n","LOGGING: writing to /opt/notebooks/hail-20240318-1332-0.2.116-cd64e0876c94.log\n"]}],"source":["# Initialise hail and spark logs? Running this cell will output a red-colored message- this is expected.\n","# The 'Welcome to Hail' message in the output will indicate that Hail is ready to use in the notebook.\n","import pyspark.sql\n","\n","config = pyspark.SparkConf().setAll([('spark.kryoserializer.buffer.max', '128')]) #This resolves an error which was initially coming up due to exceeding the allowable buffer limit size. \n","sc = pyspark.SparkContext(conf=config) \n","\n","from pyspark.sql import SparkSession\n","\n","import hail as hl\n","builder = (\n","    SparkSession\n","    .builder\n","    .enableHiveSupport()\n",")\n","spark = builder.getOrCreate()\n","hl.init(sc=sc)\n","\n","import dxpy"]},{"cell_type":"markdown","metadata":{},"source":["## Building matrix tables \n","\n","This section processes the pVCFs (the initial format of the UKBB WES data) and builds matrix tables (one per chromosome) from them. \n","\n","Here, locate WES data and import all data on one chromosome into a Hail matrix table, then write this table out to HFDS and read it back in to speed up later stages. \n","\n","### Define your variables"]},{"cell_type":"code","execution_count":2,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["# Define the chromosome you're working with \n","chr = 1\n","\n","# Define variables used in import\n","file_url = f\"file:///mnt/project/Bulk/Exome sequences/Population level exome OQFE variants, pVCF format - final release/*_c{chr}_*.vcf.gz\"\n","# For x and y the file uses the letter rather than the number, so use below line. \n","#file_url = f\"file:///mnt/project/Bulk/Exome sequences/Population level exome OQFE variants, pVCF format - final release/*_cY_*.vcf.gz\"\n","#file_url = f\"file:///mnt/project/Bulk/Exome sequences/Population level exome OQFE variants, pVCF format - final release/*_cX_*.vcf.gz\""]},{"cell_type":"markdown","metadata":{},"source":["### Import WES data into a matrix table\n","Here you want to read in the pVCFs and save them out as a matrix table, which can then be read back into hail. Saving ou the matrix table before doing any work with it helps to speed up later processes in this script. Importantly, this matrix table is saved into HFDS storage, which is storage in your compute but not saved up to your UKBB RAP project. This is useful as the initial form of this mt is large and is not required in later stages, so does not need to be copied across to your project. "]},{"cell_type":"code","execution_count":3,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-18 13:33:44.749 Hail: INFO: scanning VCF for sortedness...\n","2024-03-18 14:03:33.057 Hail: INFO: Coerced sorted VCF - no additional import work to do\n","2024-03-18 15:11:49.564 Hail: INFO: wrote matrix table with 2283839 rows and 469835 columns in 18190 partitions to ./chr_1_initial_mt.mt\n"]}],"source":["a=hl.import_vcf(file_url, \n","                 force_bgz=True,\n","                 reference_genome='GRCh38',\n","                 array_elements_required=False).write(f\"./chr_{chr}_initial_mt.mt\", overwrite=True)"]},{"cell_type":"markdown","metadata":{},"source":["Above, the initial matrix table is saved out only into HFDS storage, but not across to your project. "]},{"cell_type":"code","execution_count":4,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Num partitions: 18190\n","----------------------------------------\n","Global fields:\n","    None\n","----------------------------------------\n","Column fields:\n","    's': str\n","----------------------------------------\n","Row fields:\n","    'locus': locus<GRCh38>\n","    'alleles': array<str>\n","    'rsid': str\n","    'qual': float64\n","    'filters': set<str>\n","    'info': struct {\n","        AF: array<float64>, \n","        AQ: array<int32>, \n","        AC: array<int32>, \n","        AN: int32\n","    }\n","----------------------------------------\n","Entry fields:\n","    'GT': call\n","    'RNC': array<str>\n","    'DP': int32\n","    'AD': array<int32>\n","    'GQ': int32\n","    'PL': array<int32>\n","----------------------------------------\n","Column key: ['s']\n","Row key: ['locus', 'alleles']\n","----------------------------------------\n"]}],"source":["mt=hl.read_matrix_table(f\"./chr_{chr}_initial_mt.mt\")\n","print(f\"Num partitions: {mt.n_partitions()}\")\n","# Then check if it looks as you'd expect\n","mt.describe()"]},{"cell_type":"markdown","metadata":{},"source":["## Genotype quality control\n","Quality control (QC) is then run on these matrix tables. The resulting matrix table is then filtered for only genotype entries (to save space) and then saved out. \n","\n","### Initial QC\n","Some early genotype QC can be run before the initial save of the matrix tables so you don't need to save such a large file. Note that if you do this, you may need more memory/greater computing power and it may take longer than the predicted stages from the table. \n","\n","#### Early QC and splitting multi-allelic sites\n","Firstly, sites labelled in the pVCFs by GLNexus as ‘mono-allelic’ were removed. These are defined as sites representing an alternate allele region with multiple variants that ‘could not be unified into non-overlapping multi-allelic sites’. These typically make up < 1% of sites."]},{"cell_type":"code","execution_count":5,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["n original variants in chromosome 1: 2283839\n","n variants in chromosome 1 following removal of those labelled as monoallelic: 2259918\n","n variants in chromosome 1 following removal of those with more than 6 alleles: 2256227\n","n variants in chromosome 1 following splitting of multiallelic sites: 2631192\n","Early variant QC complete\n"]}],"source":["# First check initial variant counts\n","print(f'n original variants in chromosome {chr}: {mt.count_rows()}')\n","\n","# Remove sites labelled as 'monoallelic'\n","mt=mt.filter_rows(hl.is_missing(mt.filters))\n","print(f'n variants in chromosome {chr} following removal of those labelled as monoallelic: {mt.count_rows()}')\n","\n","# Remove sites with > 6 alleles \n","mt = mt.filter_rows(mt.alleles.length() <= 6)\n","print(f'n variants in chromosome {chr} following removal of those with more than 6 alleles: {mt.count_rows()}')\n","\n","# Split multi-allelic sites \n","mt = hl.split_multi_hts(mt)\n","print(f'n variants in chromosome {chr} following splitting of multiallelic sites: {mt.count_rows()}')\n","print('Early variant QC complete')"]},{"cell_type":"markdown","metadata":{},"source":["### Genotype QC\n","\n","Here, you can run genotype-level quality control to filter the low quality genotypes out. Saving after filtering on genotype and removing additional genotype information means the table you save out is significantly smaller than if you save out before this point. \n","\n","\n","Here, to remove genotypes based on the following criteria: \n","\n","- Unusual allele balance. Removed genotypes listed as: homozygous reference with > 10% alternate allele reads; homozygous alternate with > 10% reference allele reads; or heterozygous without a reference alternate allele balance of around 1:1 (remove genotypes with alternate allele proportion of < 25% or > 75%).\n","- Depth of < 10 \n","- Genotype quality of < 30\n","\n","\n","Subsequently, sites with no variants remaining are removed. Entries within the matrix table are then filtered to keep only the genotype (removing metrics such as depth and genotype quality), making the size of the matrix tables more computationally manageable. "]},{"cell_type":"code","execution_count":6,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Filtering on allele balance completed\n","Filtering on depth completed\n","Filtering on genotype quality completed\n"]}],"source":["# Filtering on allele balance\n","## Set the allele balance variable \n","# Create allele balance variable\n","ab=mt.AD[1]/hl.sum(mt.AD)\n","# Below removing: genotypes listed as homozygous reference but with >10 alternate allele reads | genotypes listed as homozgyous without a reference:alternate of ~1:1 | and genotypes listed as homozygous alternate with over 10% reference reads\n","filter_condition_ab=((mt.GT.is_hom_ref() & (ab <= 0.1)) | (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) | (mt.GT.is_hom_var() & (ab >= 0.9)))\n","mt=mt.filter_entries(filter_condition_ab)\n","print('Filtering on allele balance completed')\n","# Remove entries with depth < 10\n","mt=mt.filter_entries(mt.DP > 10)\n","print('Filtering on depth completed')\n","# Remove entries with a GQ < 30\n","mt=mt.filter_entries(mt.GQ > 30)\n","print('Filtering on genotype quality completed')"]},{"cell_type":"code","execution_count":7,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-18 15:30:25.887 Hail: INFO: Ordering unsorted dataset with network shuffle\n","2024-03-18 16:30:01.538 Hail: INFO: wrote matrix table with 2631192 rows and 469835 columns in 36380 partitions to ./chr_1_postgenoQC_mt.mt\n"]}],"source":["mt = hl.variant_qc(mt)\n","# The next stage (filtering out non-variant sites) runs as a very slow single stage if the mt is not saved out first\n","# So best to save out at this point in parallel, enabling later stages to run in parallel too.\n","mt.write(f\"./chr_{chr}_postgenoQC_mt.mt\", overwrite=True)"]},{"cell_type":"markdown","metadata":{},"source":["When saving out second mt above, storage used roughly doubles. "]},{"cell_type":"code","execution_count":8,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["n variants in chr 1 following genotype QC and removal of non-variant rows: 2455972\n","Genotype QC complete\n"]}],"source":["mt=hl.read_matrix_table(f\"./chr_{chr}_postgenoQC_mt.mt\")\n","# Remove variants with no alleles left following genotype QC \n","# When run before being saved as an mt first, this was a v slow single stage, but now runs in parallel.  \n","mt = mt.filter_rows((mt.variant_qc.AF[0] == 0.0) | (mt.variant_qc.AF[0] == 1.0), keep = False)\n","print(f'n variants in chr {chr} following genotype QC and removal of non-variant rows: {mt.count_rows()}')\n","print('Genotype QC complete')"]},{"cell_type":"markdown","metadata":{},"source":["### Recalculate sample and variant QC metrics following genotype QC\n","\n","These metrics should have improved following the removal of low quality genotypes.\n","\n","Once these table have been read out, you should investigate them in R (as per scripts on git) to define filters you'd like to use for variant and sample level QC."]},{"cell_type":"code","execution_count":9,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------\n","Global fields:\n","    None\n","----------------------------------------\n","Column fields:\n","    's': str\n","----------------------------------------\n","Row fields:\n","    'locus': locus<GRCh38>\n","    'alleles': array<str>\n","    'rsid': str\n","    'qual': float64\n","    'filters': set<str>\n","    'info': struct {\n","        AF: array<float64>, \n","        AQ: array<int32>, \n","        AC: array<int32>, \n","        AN: int32\n","    }\n","    'a_index': int32\n","    'was_split': bool\n","    'variant_qc': struct {\n","        dp_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        gq_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        AC: array<int32>, \n","        AF: array<float64>, \n","        AN: int32, \n","        homozygote_count: array<int32>, \n","        call_rate: float64, \n","        n_called: int64, \n","        n_not_called: int64, \n","        n_filtered: int64, \n","        n_het: int64, \n","        n_non_ref: int64, \n","        het_freq_hwe: float64, \n","        p_value_hwe: float64, \n","        p_value_excess_het: float64\n","    }\n","----------------------------------------\n","Entry fields:\n","    'GT': call\n","    'RNC': array<str>\n","    'DP': int32\n","    'AD': array<int32>\n","    'GQ': int32\n","    'PL': array<int32>\n","----------------------------------------\n","Column key: ['s']\n","Row key: ['locus', 'alleles']\n","----------------------------------------\n"]}],"source":["# Calculate variant QC \n","mt=hl.variant_qc(mt)\n","mt.describe()"]},{"cell_type":"code","execution_count":10,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------\n","Global fields:\n","    None\n","----------------------------------------\n","Column fields:\n","    's': str\n","    'sample_qc': struct {\n","        dp_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        gq_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        call_rate: float64, \n","        n_called: int64, \n","        n_not_called: int64, \n","        n_filtered: int64, \n","        n_hom_ref: int64, \n","        n_het: int64, \n","        n_hom_var: int64, \n","        n_non_ref: int64, \n","        n_singleton: int64, \n","        n_snp: int64, \n","        n_insertion: int64, \n","        n_deletion: int64, \n","        n_transition: int64, \n","        n_transversion: int64, \n","        n_star: int64, \n","        r_ti_tv: float64, \n","        r_het_hom_var: float64, \n","        r_insertion_deletion: float64\n","    }\n","----------------------------------------\n","Row fields:\n","    'locus': locus<GRCh38>\n","    'alleles': array<str>\n","    'rsid': str\n","    'qual': float64\n","    'filters': set<str>\n","    'info': struct {\n","        AF: array<float64>, \n","        AQ: array<int32>, \n","        AC: array<int32>, \n","        AN: int32\n","    }\n","    'a_index': int32\n","    'was_split': bool\n","    'variant_qc': struct {\n","        dp_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        gq_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        AC: array<int32>, \n","        AF: array<float64>, \n","        AN: int32, \n","        homozygote_count: array<int32>, \n","        call_rate: float64, \n","        n_called: int64, \n","        n_not_called: int64, \n","        n_filtered: int64, \n","        n_het: int64, \n","        n_non_ref: int64, \n","        het_freq_hwe: float64, \n","        p_value_hwe: float64, \n","        p_value_excess_het: float64\n","    }\n","----------------------------------------\n","Entry fields:\n","    'GT': call\n","    'RNC': array<str>\n","    'DP': int32\n","    'AD': array<int32>\n","    'GQ': int32\n","    'PL': array<int32>\n","----------------------------------------\n","Column key: ['s']\n","Row key: ['locus', 'alleles']\n","----------------------------------------\n"]}],"source":["# Calculate sample QC \n","mt=hl.sample_qc(mt)\n","mt.describe()"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-18 16:37:01.858 Hail: INFO: merging 36381 files totalling 1.2G...\n","2024-03-18 16:38:05.538 Hail: INFO: while writing:\n","    chr_1_post_geno_QC_var_qc.csv\n","  merge time: 1m3.7s\n"]},{"name":"stdout","output_type":"stream","text":["Variant QC table written\n"]}],"source":["var_qc=mt.variant_qc\n","var_qc.export(f\"chr_{chr}_post_geno_QC_var_qc.csv\", delimiter=\",\")\n","print('Variant QC table written')"]},{"cell_type":"code","execution_count":12,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-18 16:39:21.500 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.\n","    To preserve matrix table column order, first unkey columns with 'key_cols_by()'\n","2024-03-18 16:53:21.209 Hail: INFO: Coerced sorted dataset\n","2024-03-18 16:53:22.975 Hail: INFO: merging 17 files totalling 250.5M...\n"]},{"name":"stdout","output_type":"stream","text":["Sample QC table written\n"]},{"name":"stderr","output_type":"stream","text":["2024-03-18 16:53:23.967 Hail: INFO: while writing:\n","    chr_1_post_geno_QC_sample_qc.csv\n","  merge time: 991.044ms\n"]}],"source":["qc_tb=mt.cols()\n","qc_tb.export(f\"chr_{chr}_post_geno_QC_sample_qc.csv\", delimiter=\",\")\n","print('Sample QC table written')"]},{"cell_type":"markdown","metadata":{},"source":["Copy the output files (which are currently in hdfs storage into your current environment, and then copy them up to your project on the RAP. \n"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["%%bash\n","hdfs dfs -get ./*.csv ./"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["%%bash\n","# Upload these to that dir within your project\n","dx upload ./*.csv --destination ./sample_and_var_QC_metrics/ "]},{"cell_type":"markdown","metadata":{},"source":["### Save out a matrix table here\n","\n","At this stage, you can remove genotype data which is no longer required. Removing this data saves lots of space! Then save out a matrix table for use in future steps. \n","\n","As lots of information has been removed, the partitions are now larger than they need to be. Given that later stages of this process run across one partition at a time, having fewer partitions will speed up processing. The n partitions you want should be set manually, and I usually used around 1/3 as many partitions as there were when the mt was originally read in (reported in code box [4]). \n","'shuffle=false' in the repartition command means new partitions will be formed just from the merging of previous partitions. This means some partitions will be uneven sizes, so not all tasks on partitions will run at the same time in the future. Remove this if you require even partitions, although this makes the repartitioning much more computationally demanding. "]},{"cell_type":"code","execution_count":15,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------\n","Global fields:\n","    None\n","----------------------------------------\n","Column fields:\n","    's': str\n","    'sample_qc': struct {\n","        dp_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        gq_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        call_rate: float64, \n","        n_called: int64, \n","        n_not_called: int64, \n","        n_filtered: int64, \n","        n_hom_ref: int64, \n","        n_het: int64, \n","        n_hom_var: int64, \n","        n_non_ref: int64, \n","        n_singleton: int64, \n","        n_snp: int64, \n","        n_insertion: int64, \n","        n_deletion: int64, \n","        n_transition: int64, \n","        n_transversion: int64, \n","        n_star: int64, \n","        r_ti_tv: float64, \n","        r_het_hom_var: float64, \n","        r_insertion_deletion: float64\n","    }\n","----------------------------------------\n","Row fields:\n","    'locus': locus<GRCh38>\n","    'alleles': array<str>\n","    'rsid': str\n","    'qual': float64\n","    'filters': set<str>\n","    'info': struct {\n","        AF: array<float64>, \n","        AQ: array<int32>, \n","        AC: array<int32>, \n","        AN: int32\n","    }\n","    'a_index': int32\n","    'was_split': bool\n","    'variant_qc': struct {\n","        dp_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        gq_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        AC: array<int32>, \n","        AF: array<float64>, \n","        AN: int32, \n","        homozygote_count: array<int32>, \n","        call_rate: float64, \n","        n_called: int64, \n","        n_not_called: int64, \n","        n_filtered: int64, \n","        n_het: int64, \n","        n_non_ref: int64, \n","        het_freq_hwe: float64, \n","        p_value_hwe: float64, \n","        p_value_excess_het: float64\n","    }\n","----------------------------------------\n","Entry fields:\n","    'GT': call\n","----------------------------------------\n","Column key: ['s']\n","Row key: ['locus', 'alleles']\n","----------------------------------------\n"]}],"source":["# Drop all entries except the genotype\n","mt=mt.select_entries(mt.GT)\n","mt.describe()\n","mt=mt.repartition(6000, shuffle=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["# Save out this data table using DNAX \n","# Define database and MT names\n","# Note: It is recommended to only use lowercase letters for the database name.\n","# If uppercase lettering is used, the database name will be lowercased when creating the database.\n","db_name = f\"ukbb_test_wes_matrix_tables_feb23_test\"\n","mt_name = f\"chromosome_{chr}_post_genoqc_final.mt\""]},{"cell_type":"code","execution_count":17,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CREATE DATABASE IF NOT EXISTS ukbb_test_wes_matrix_tables_feb23_test LOCATION 'dnax://'\n","++\n","||\n","++\n","++\n","\n"]}],"source":["# Create database in DNAX\n","stmt = f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION 'dnax://'\"\n","print(stmt)\n","spark.sql(stmt).show()"]},{"cell_type":"code","execution_count":18,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-18 17:24:57.610 Hail: INFO: wrote matrix table with 2455972 rows and 469835 columns in 6000 partitions to dnax://database-GgPbpq8J637bkp84VQyQ83X9/chromosome_1_post_genoqc_final.mt\n"]}],"source":["# Store MT in DNAX\n","import dxpy\n","\n","# Find database ID of newly created database using dxpy method\n","db_uri = dxpy.find_one_data_object(name=f\"{db_name}\", classname=\"database\")['id']\n","url = f\"dnax://{db_uri}/{mt_name}\" # Note: the dnax url must follow this format to properly save MT to DNAX\n","\n","# Before this step, the Hail MatrixTable is just an object in memory. To persist it and be able to access \n","# it later, the notebook needs to write it into a persistent filesystem (in this case DNAX).\n","mt.write(url) # Note: output should describe size of MT (i.e. number of rows, columns, partitions) "]},{"cell_type":"markdown","metadata":{},"source":["This saved out matrix table can then be read into JupyterLab for the next stages of processing. First, post genotype QC sample and variant level QC metrics should be read out of the mt, examined and cut-offs defined. Once these have been defined, they can be applied to the matrix table to leave only high quality samples and variants in the table. \n","\n","## Check the mt you've saved out\n","Check the mt you jsut wrote can be read back in, and ensure the count is as you'd expect. "]},{"cell_type":"code","execution_count":19,"metadata":{"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(2455972, 469835)\n","----------------------------------------\n","Global fields:\n","    None\n","----------------------------------------\n","Column fields:\n","    's': str\n","    'sample_qc': struct {\n","        dp_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        gq_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        call_rate: float64, \n","        n_called: int64, \n","        n_not_called: int64, \n","        n_filtered: int64, \n","        n_hom_ref: int64, \n","        n_het: int64, \n","        n_hom_var: int64, \n","        n_non_ref: int64, \n","        n_singleton: int64, \n","        n_snp: int64, \n","        n_insertion: int64, \n","        n_deletion: int64, \n","        n_transition: int64, \n","        n_transversion: int64, \n","        n_star: int64, \n","        r_ti_tv: float64, \n","        r_het_hom_var: float64, \n","        r_insertion_deletion: float64\n","    }\n","----------------------------------------\n","Row fields:\n","    'locus': locus<GRCh38>\n","    'alleles': array<str>\n","    'rsid': str\n","    'qual': float64\n","    'filters': set<str>\n","    'info': struct {\n","        AF: array<float64>, \n","        AQ: array<int32>, \n","        AC: array<int32>, \n","        AN: int32\n","    }\n","    'a_index': int32\n","    'was_split': bool\n","    'variant_qc': struct {\n","        dp_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        gq_stats: struct {\n","            mean: float64, \n","            stdev: float64, \n","            min: float64, \n","            max: float64\n","        }, \n","        AC: array<int32>, \n","        AF: array<float64>, \n","        AN: int32, \n","        homozygote_count: array<int32>, \n","        call_rate: float64, \n","        n_called: int64, \n","        n_not_called: int64, \n","        n_filtered: int64, \n","        n_het: int64, \n","        n_non_ref: int64, \n","        het_freq_hwe: float64, \n","        p_value_hwe: float64, \n","        p_value_excess_het: float64\n","    }\n","----------------------------------------\n","Entry fields:\n","    'GT': call\n","----------------------------------------\n","Column key: ['s']\n","Row key: ['locus', 'alleles']\n","----------------------------------------\n"]}],"source":["# Check your mt can be read in for later scripts\n","b=hl.read_matrix_table(f\"dnax://database-GgPbpq8J637bkp84VQyQ83X9/chromosome_{chr}_post_genoqc_final.mt\")\n","print(b.count())\n","b.describe()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"toc-autonumbering":false,"toc-showcode":false,"toc-showmarkdowntxt":false},"nbformat":4,"nbformat_minor":4}
